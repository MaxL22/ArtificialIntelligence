\section{Evolutionary Algorithms}

An optimization problem can be described by a triple $(\Omega, f, \prec)$, where $\Omega$ is the search space, $f$ is an objective function in the form $f: \Omega \rightarrow \mathbb{R}$ and $\prec$ a comparison relationship. We want to find an element $x \in \Omega$ which optimizes the function in the search space.\\

\subsection{Metaheuristics}
Metaheuristics are computational techniques which aim to solve approximately CO problems in several iterations, usually applied to $\mathcal{NP}$ problems. They often operate by improving upon a set of candidate solutions.\\
 
 \paragraph{Local Search Methods:} Given $(\Omega, f, \prec)$, starting from a solution, we can search around it in a neighborhood searching for a local maximum. The search can proceed via gradient, if $f$ is differentiable (gradient ascent/descent), otherwise evaluating, either exhaustively or heuristically, the points around the solution (steepest descent/hill climbing).\\
 
 \paragraph{Simulated Annealing:} We want a higher chance of moving from worse to better than the opposite. Starting from a solution, random variants are created and evaluated: they always replace the current reference solution if better, if worse they're accepted with a probability dependent on how much worse they are and a temperature parameter, which decreases with time.
 
 \paragraph{Threshold Accepting:} Similar to simulated annealing, but with a limit on quality degradation of the solution.
 
 \paragraph{Gread Deluge Algorithm:} Similar to before, but solutions are always accepted within an absolute bound.
 
 \paragraph{Record-to-Record travel:} Similar to Gread Deluge, but the threshold is relative to the best solution found instead of absolute.\\
 
 \paragraph{Tabu Search:} Considers history, forbids exploring previously considered solution candidates, usually for a set amount of time (tenure).\\
 
 There's more, but I'm kinda bored and know them already. I don't think they're too useful so I'm just non gonna write them.\\
 
 \subsection{Actual Evolutionary Algorithms}
 
 The idea behind evolutionary computing is to mimic biological evolution, positive traits obtained by random mutations should be favored by natural selection. Starting from a population, we want to see how it evolves.\\
 
 For  an evolutionary algorithm we need: 
 \begin{itemize}
 	\item An encoding of the solutions, dependent on the problem
 	\item A method to create an initial population (usually random or by a simple heuristic)
 	\item An evaluation function to evaluate the fitness of each individual 
 	\item A selection criteria, in relation with the evaluation function, needed to select individuals that should go into successive generations
 	\item A set of genetic operators to modify chromosomes (pieces of the encoding); the most common two are mutation and crossover
 	\item Parameters for population size, mutation, \dots
 	\item Termination criteria
 \end{itemize}
 
EAs separate the search space $\Omega$ into genotype $\mathcal{G}$ for the encoding on which genetic operators act, and phenotype $\mathcal{F}$, on which the fitness function $f$ is defined. Fitness is what determines the chance of survival of an individual, and the strength of fitness in such probability is called "selective pressure", the higher the pressure the more good individuals are favored, it determines the balance between intensification on already found solutions and exploration of the search space.\\

\subsubsection{Encoding}
The encoding of the solutions should allow visiting the whole search space, be similar for similar candidates and the fitness function should return similar values for similar candidates. These are highly dependent on the problem and might not always be easily satisfiable.\\
The encoding/decoding process should not be too computationally expensive.\\

\subsubsection{Selection}
At each generation, we want to create a new population starting from the previous one. There are various methods to select the individuals:
\begin{itemize}
	\item Roulette-wheel selection: random selection but each individual has probability proportional to its fitness; it's simple but has no guarantees, and might soon lead to stagnation if candidates end up having similar fitness
	\item Rank-based selection: sort the individual by descending fitness, give probability to each of them based on their rank in the list and extract at random; it's more computationally complex due to the sorting but allows the decoupling of fitness value and selection probability
	\item Tournament selection: a number of subsets of the population is chosen and there's a "tournament" for each subset, the best individual from each subset is kept
	\item Elitist: keep only the best
	\item Crowding: Individuals are replaced by similar ones
\end{itemize}

\subsubsection{Genetic Operators}
To generate new elements starting from a population, we apply genetic operators on some or all the individuals. The operators can start from one, two or multiple parents.\\

\paragraph{Mutation:} One-parent operator, it introduces changes on the genome of the candidate considered, useful for introducing biodiversity in the genetic pool. The specific mutation applied usually depends on the encoding of the solutions, but common ones are swaps of values, shifts and permutations.\\

\paragraph{Crossover:} Two-parents operator, given two solutions it creates a new one: 
\begin{itemize}
	\item One-point crossover: determine a random position inside the encoded solution and swap the "tails"
	\item N-point crossover: determine N random position and swap all odd(/even) parts
	\item Uniform crossover: randomly extract a mask that determines whether or not to swap each gene in the encoding
	\item Shuffle crossover: permute the genes randomly, one-point crossover, then un-mix the genes
\end{itemize}

The crossover can also be a multiple-parents operator: diagonal crossover, given $k$ parents, choose $k-1$ points and shift the pieces diagonally.\\

\subsubsection{Introns}
EAs tend to produce increasingly more complicated candidates, in part due to the presence of introns, pieces of DNA that carry no information and thus, for our case, that have no effect on the fitness. Mutation/recombination on introns has no effect.\\
Introns can easily grow in number and complexity since they cause no direct penalty on the fitness function, but it's better to prevent introns as much as possible given that they increase the time needed to find significant data and make solution harder to interpret.\\

Some solutions could be: modifying the fitness function to penalize introns, change the recombination phase in order to prevent them or prioritizing simple chromosomes, thus avoiding too many introns (sometimes at the expense of genetic diversity).\\

\subsection{Swarm and Population based optimization}

Swarm intelligence (SI) is a computational approach inspired by the collective behavior of social animals and insects, entailing simple individuals following simple rules with only local interactions that lead to a collective behavior that cannot be concluded based on the rules of a single individual. This concept can be used to develop multi-agent intelligent systems, able to perform a cooperative behavior. Single individuals can exchange information without a central control unit. A population of individuals moves in the search space.\\

\subsubsection{Particle Swarm Optimization PSO}

Inspired to the biological pattern of food research of birds and fishes. A swarm of candidates solutions aggregates information to guide the search. Each candidate updates its position based on personal and global memory. Informations are shared and the collective memory is used to find solutions.\\
Each individual  is a possible candidate for being in the solution.\\

\subsubsection{Ant Colony Optimization ACO}

Inspired to the biological pattern of the ants searching for food. More promising paths are marked with pheromones, the individuals exchange information by editing the environment. Each candidate leaves a trail that modifies the search space for all others. Each individual is a possible candidate for being in the solution.\\

\subsubsection{Population-based Incremental Learning PBIL}

A population of individuals is generated randomly according to a probability distribution. Instead of explicitly conserving the memory of individuals, PBIL focuses on maintaining the statistics of the population.\\

As a recombination operator is used the uniform crossover. The selection process only chooses individuals that improve the statistics of the population. The mutation is a simple bit flip.\\

The distinctive feature is the learning rate, which tunes the possibility of movement of the individuals in the space. This changes in the time and is reduced with the number of iteration. This allows great mobility initially, eventually reducing when an optimum is found. \\
%Some problems with this strategy:
%\begin{itemize}
%	\item The algorithm can learn some accidental dependency between the chromosomes of the individuals and 
%	considering the single bits individually isoled from the others.
%	2. The same statistics representation of the population can be used for dif-
%	ferent populations
%\end{itemize}

\subsection{Theoretical foundations}

\subsubsection{Schema Theorem}
To analyze why evolutionary algorithms work, we can look at the schema theorem. Since these algorithms work on bit strings, we can consider only binary chromosomes, more precisely, we consider schemata partly specified binary chromosomes. We then investigate how the number of chromosomes matching a schema evolve over several generations. The objective is to give a rough stochastic statement that describes how these algorithms explore the search space.\\

\begin{definition}
	A \textit{Schema} $h$ is a character string of length $L$ over the alphabet $\{0,1,\ast\}$, $h \in \{0,1,\ast\}^L$. The $\ast$ is a wildcard character.\\
\end{definition}

\begin{definition}
	\textit{Matching}, a binary chromosome $c \in \{0,1\}^L$ \textit{matches} a schema $h \in \{0,1,\ast\}^L$, written as $c \triangleleft h$ iff it coincides with $h$ at all positions where $h$ is 0 or 1, not taking into account $\ast$.\\
\end{definition}

We want to measure the effect of selection on the fitness. The individuals matching a schema in the next generation will in some way proportional to the measure of fitness of the schema. As a measure, mean relative fitness is usually chosen.\\

To calculate the influence of mutations we need to measure the probability of preserving the schema even after the mutation.\\

\begin{definition}
	The \textit{Order of a schema} $h$ is the number of zeros and ones in $h$, $\ord(h) = \# 0 + \# 1 = L - \#\ast$.\\
\end{definition}


Considering one-point crossover, the probability that mutation preserves the schema $h$ is $(1-p_m)^{\ord(h)}$, where $p_m$ is the "mutation probability".\\
So, the probability of preserving the schema is measured w.r.t. the defining length.\\

\begin{definition}
	The \textit{Defining length} of a schema $h$ ($\dl(h)$) is the difference between the position  of the last 0(1) and the first 0(1) in $h$.\\
\end{definition}

\begin{definition}
	The probability that a chromosome is cut in such a way that some of the fixed characters of a schema lie on one side of a cut and some on the others is $\dl(h)/L-1$.\\
\end{definition}

\begin{definition}
	$N(h,t)$ is the expected value of the number of chromosomes that match the schema $h$ during the $t$-th generation.\\
\end{definition}

\begin{definition}
	$N(h, t + \Delta t_s)$ is the expected value of the number of chromosomes that match the schema $h$ during the $t$-th generation after selection.\\
\end{definition}

\begin{definition}
	$N(h, t + \Delta t_s + \Delta t_x)$ is the expected value of the number of chromosomes that match the schema $h$ during the $t$-th generation after selection and crossover.\\
\end{definition}

\begin{definition}
	$N(h, t + \Delta t_s + \Delta t_x + \Delta t_m) = N(h, t+1)$ is the expected value of the number of chromosomes that match the schema $h$ during the $t$-th generation after selection, crossover and mutation. The various $\Delta$ will depend on the probabilities of the single operators defined.\\
\end{definition}

\paragraph{Schema theorem:} It says that the schemas with a fitness score above the average, a short defining length and a low order, reproduces in an approximately exponential way.\\

\subsubsection{No free lunch theorem}
The theorem says that there is no evolutionary algorithm that can be used for all problems, no other algorithm is superior to the average.\\
The "right" choice and encoding of the "correct" genetic operators depend on knowledge of the specific problem. \\

\subsection{Genetic Programming GP}

The idea behind GP is to automatically create programs able to solve problems, many of them can be seen as a search for a program; we need to connect input and output. GP is a family of evolutionary algorithms that allows the automatic creation of programs that solve problems.\\

Similarly to genetic algorithms, we need an encoding that allows representation and manipulation of the single programs. Usually, the parse tree representation is chosen, where internal nodes are operations and the leaves are variables/constants. We define the problem-specific sets: $F$ of function symbols and operators and $T$ of terminal symbols (constants and variables). GP can solve problems efficiently and effectively, if such sets are sufficient and complete, to ensure that an appropriate program can be found. Finding the smallest sets is usually $\mathcal{NP}$-hard.\\
For a boolean function the sets might be $F=\{\vee, \wedge, \neg\}$ and $T=\{x_0, \dots, x_n, 0, 1\}$.\\


The algorithm is:
\begin{enumerate}
	\item Generate an initialization population of random expressions
	\item Evaluate the fitness of said expressions
	\item Selection with one of the EA strategies
	\item Genetic operators, usually only crossover
\end{enumerate}

\subsubsection{Initialization}
We need to generate a random population. There's no fixed length for the chromosomes (the programs), so we need some parameters like maximum height and maximum number of nodes. There are various methods to initialize the parse trees:
\begin{itemize}
	\item Full: leaves occur only at maximum depth, resulting in (full) symmetric trees
	\item Grow: at any level there's uniform probability of having either an internal node or leaf, resulting in asymmetric trees
	\item Ramp-half-and-half: combines both methods across different depths to increase diversity
\end{itemize}

\subsubsection{Genetic Operators}
The evolution of the population takes place via genetic operators. The 3 most important are mutation, crossover and clonal reproduction. A typical crossover operator is exchanging two sub-expressions, i.e., choosing an internal node and switching the sub-trees. A mutation could be exchanging a sub-expression (sub-tree) with one randomly generated.\\

\subsection{Evolutionary Strategies ES}

In an ES we try to optimize the entire evaluation process instead of the single individual. We need parameters that depend on the choice of genetic operators.\\
We consider an optimization problem as a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ to minimize. Chromosomes (solutions encoding) are represented by a real-valued array. We then use mutation to move the vector inside the search space, modifying it by adding a vector randomly taken from a normal distribution. Only the best (mutated) elements proceed in the subsequent generation (elitism). Possible selection approaches are:
\begin{itemize}
	\item Plus strategy: selection works on both mutated and non-mutated elements (fitness can only improve, can get stuck in local optima)
	\item Comma strategy: generate a lot of mutated individuals and choose solely among them (increases diversity)
\end{itemize}

We can dynamically adapt the variance of the added vector in order to optimize convergence; small variance leads to small changes (exploitation), high variance leads to big changes (exploration). \\

The variance is "good" when $1/5$ of the mutated elements are successful, i.e., have better fitness.\\
We can also save, along with each chromosome, its variance, in order to determine which ones have "bad" variance and thus will lead to worse descendants; we can then remove them from subsequent generations.\\

%TODO
% Multi-criteria optimization



